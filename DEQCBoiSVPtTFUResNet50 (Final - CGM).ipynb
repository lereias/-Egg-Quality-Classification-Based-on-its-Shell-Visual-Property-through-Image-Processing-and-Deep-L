{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d37d7c1",
   "metadata": {},
   "source": [
    "# Duck Egg Quality Classification Based on its Shell Visual Property through Image Processing and Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bab25e6",
   "metadata": {},
   "source": [
    "Group CGM: <br>\n",
    "\n",
    "Caguioa, JV Bryan <br>\n",
    "Guinto, Ryhle Nodnyhlson <br>\n",
    "Mesias, Lee Reuben"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976e157b",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed03b6",
   "metadata": {},
   "source": [
    "https://thedatafrog.com/en/articles/image-recognition-transfer-learning/ <br>\n",
    "https://medium.com/the-owl/k-fold-cross-validation-in-keras-3ec4a3a00538 <br>\n",
    "https://www.datacamp.com/community/tutorials/convolutional-neural-networks-python <br>\n",
    "https://keras.io/examples/vision/grad_cam/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c29596",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf98030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T18:54:47.963474Z",
     "start_time": "2021-09-28T18:54:44.732526Z"
    }
   },
   "outputs": [],
   "source": [
    "import os                                              # define and move to dataset and model directory\n",
    "import shutil                                          # duplicate images and delete directories\n",
    "import sys                                             # exit out code     \n",
    "import cv2                                             \n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import tensorflow as tf                                 #for callbacks\n",
    "import seaborn as sn\n",
    "import pandas as pd \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt                         #histograph/confusion matrix\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold     #k-fold cross validation\n",
    "from sklearn.model_selection import train_test_split    #split the dataset to train-test\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator      # data augmentation\n",
    "from tensorflow.keras.preprocessing import image as im                   #load image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input      #preprocess image                                                               \n",
    "\n",
    "#measure performance metrics\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve, auc       \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from scipy import interp                            #interpolation needed for plotting all classes into one roc plot\n",
    "from itertools import cycle                         #for the color of lines on that multiclass plot\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Grad-CAM\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1cd4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the directory where the data is stored\n",
    "directory = \"C:/Users/user/Desktop/Dataset\"\n",
    "#directory where set of models per training are stored\n",
    "gen_dir = \"C:/Users/user/Desktop/Model\"\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "if not os.path.exists(gen_dir):\n",
    "    os.makedirs(gen_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab583ca",
   "metadata": {},
   "source": [
    "## Put Dataset in a Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11561c6",
   "metadata": {},
   "source": [
    "The directory of the images are put in a dataframe in order to prepare for splitting the dataset into train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef7d10b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T18:54:47.979198Z",
     "start_time": "2021-09-28T18:54:47.965672Z"
    }
   },
   "outputs": [],
   "source": [
    "totaldsdirect = directory + \"/CombinedTwoAngleEggs\"\n",
    "\n",
    "image = []\n",
    "\n",
    "if not os.path.exists(totaldsdirect):\n",
    "    print(\"Directory does not exist. Please run the code on data cleaning notebook.\")\n",
    "    sys.exit()\n",
    "\n",
    "#label the images\n",
    "for x in os.listdir(totaldsdirect):\n",
    "    totaldsdirect2 = totaldsdirect + \"/\" + x\n",
    "    for y in os.listdir(totaldsdirect2):\n",
    "        if x == \"Balut-penoy2\":\n",
    "            lbl = \"0\"\n",
    "        elif x == \"Salted egg2\":\n",
    "            lbl = \"1\"\n",
    "        else:\n",
    "            lbl = \"2\"\n",
    "                 \n",
    "        #append to image array\n",
    "        image.append([totaldsdirect2 + \"/\" + y,lbl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b6f4d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T18:54:47.995170Z",
     "start_time": "2021-09-28T18:54:47.980207Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(image,columns=['filename','label'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af8ee7",
   "metadata": {},
   "source": [
    "Classes: <br>\n",
    "0 - Balut/Penoy <br> \n",
    "1 - Salted <br>\n",
    "2 - Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45903ac",
   "metadata": {},
   "source": [
    "## Split Data to train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b4c490",
   "metadata": {},
   "source": [
    "We will need to split the dataset to train and test set (90:10). Only the train set will be used for Hyperparameter tuning with Stratified K-fold Cross Validation. Test set will be used for model prediction and model evaluation. Stratify is used in order to split the data in equal numbers under each class specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d3e41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T09:49:28.996176Z",
     "start_time": "2021-09-29T09:49:28.925443Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data to train and test\n",
    "X = df[['filename']]\n",
    "y = df[['label']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=420)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283bbdd6",
   "metadata": {},
   "source": [
    "Save the images in the split sets to train and test set folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad4898",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T09:49:30.830230Z",
     "start_time": "2021-09-29T09:49:29.744432Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#folder to store the train and test set folders\n",
    "fortraintest = directory + \"/ToBeUsed\"\n",
    "if not os.path.exists(fortraintest):\n",
    "    os.makedirs(fortraintest)\n",
    "\n",
    "#train set folder\n",
    "tr = fortraintest + \"/train_set\"\n",
    "if not os.path.exists(tr):\n",
    "    os.makedirs(tr)\n",
    "else:\n",
    "    #remove previous folder and create new one\n",
    "    shutil.rmtree(tr)\n",
    "    os.makedirs(tr)\n",
    "    \n",
    "#test set folder\n",
    "te = fortraintest + \"/test_set\"\n",
    "if not os.path.exists(te):\n",
    "    os.makedirs(te)\n",
    "else:\n",
    "    #remove previous folder and create new one\n",
    "    shutil.rmtree(te)\n",
    "    os.makedirs(te)\n",
    "\n",
    "#copying the images to the train set folder\n",
    "for index, row in X_train.iterrows():\n",
    "    trcpy = shutil.copy2(row['filename'], tr)\n",
    "    X_train.loc[X_train['filename']==row['filename'],'filename']=trcpy #change to directory where image is copied\n",
    "    \n",
    "#copying the images to the test set folder\n",
    "for index, row in X_test.iterrows():\n",
    "    tecpy = shutil.copy2(row['filename'], te)\n",
    "    X_test.loc[X_test['filename']==row['filename'],'filename']=tecpy #change to directory where image is copied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b96052",
   "metadata": {},
   "source": [
    "### Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d7592",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T09:49:31.169745Z",
     "start_time": "2021-09-29T09:49:31.140253Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.concat([X_train,y_train],axis=1)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61160bbf",
   "metadata": {},
   "source": [
    "To check whether or not there are equal amounts of images per class in the train set, we have done what is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b243cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "counto = [0,0,0]\n",
    "\n",
    "counto_count = 0\n",
    "\n",
    "for x in df2['label'].unique():\n",
    "    for y in df2['label']:\n",
    "        if x == y:\n",
    "            if counto_count == 0:\n",
    "                counto[0] += 1\n",
    "            elif counto_count == 1:\n",
    "                counto[1] += 1\n",
    "            else:\n",
    "                counto[2] += 1\n",
    "                \n",
    "    counto_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b9a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb92dc",
   "metadata": {},
   "source": [
    "### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f73ea82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3 = pd.concat([X_test,y_test],axis=1)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922d8d09",
   "metadata": {},
   "source": [
    "Same with the train set, we checked whether or not there is an equal distribution of images in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c174f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "counto = [0,0,0]\n",
    "\n",
    "counto_count = 0\n",
    "\n",
    "for x in df3['label'].unique():\n",
    "    for y in df3['label']:\n",
    "        if x == y:\n",
    "            if counto_count == 0:\n",
    "                counto[0] += 1\n",
    "            elif counto_count == 1:\n",
    "                counto[1] += 1\n",
    "            else:\n",
    "                counto[2] += 1\n",
    "                \n",
    "    counto_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba53ab2",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a5dd7d",
   "metadata": {},
   "source": [
    "### Create Plot History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddf84bd",
   "metadata": {},
   "source": [
    "Plot history will display the training and validation accuracy and loss in every epoch per fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e26f5b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T18:54:48.742719Z",
     "start_time": "2021-09-28T18:54:48.728430Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(history, yrange):\n",
    "    #Plot loss and accuracy as a function of the epoch,\n",
    "    #for the training and validation datasets.\n",
    "    \n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    # Get number of epochs\n",
    "    epochs = range(len(acc))\n",
    "\n",
    "    plt.rcParams['axes.facecolor'] = 'white'\n",
    "    plt.rcParams[\"axes.edgecolor\"] = 'black'\n",
    "    plt.rcParams[\"axes.linewidth\"]  = 1.25\n",
    "    \n",
    "    # Plot training and validation accuracy per epoch\n",
    "    plt.plot(epochs, acc)\n",
    "    plt.plot(epochs, val_acc)\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend(['train_accuracy','val_accuracy'])\n",
    "    plt.ylim(yrange)\n",
    "    \n",
    "    # Plot training and validation loss per epoch\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(epochs, loss)\n",
    "    plt.plot(epochs, val_loss)\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend(['train_loss','val_loss'])\n",
    "    \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca04f34",
   "metadata": {},
   "source": [
    "### Save Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e38fce",
   "metadata": {},
   "source": [
    "Stratified K-fold Cross Validation will be used in getting the best hyperparameters that will be utilized to train the final model. The set of model weights produced per fold will be saved in an automatic generated folder. Every training will generate another folder to save the new set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5339dd2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T18:54:48.757896Z",
     "start_time": "2021-09-28T18:54:48.743630Z"
    }
   },
   "outputs": [],
   "source": [
    "def savemodel():\n",
    "    save_cur = \"\"  #check if there is any saved_models folder(s)\n",
    "\n",
    "    svnum = 0\n",
    "    svnum2 = 0\n",
    "\n",
    "    for x in os.listdir(gen_dir):\n",
    "        if \"saved_models\" in x:\n",
    "            save_cur = x\n",
    "            if not save_cur == \"\":\n",
    "                for word in save_cur.split(\"_\"):\n",
    "                    if word.isdigit():\n",
    "                        svnum2 = int(word)\n",
    "                        if svnum < svnum2:\n",
    "                            svnum = svnum2\n",
    "\n",
    "\n",
    "    #save_dir is where the created models using the current set of hyperparameters are added        \n",
    "    if save_cur == \"\":\n",
    "        save_dir = gen_dir + \"/saved_models_1\"\n",
    "        os.makedirs(save_dir)\n",
    "    else:\n",
    "        save_dir = gen_dir + \"/saved_models_\" + str(svnum + 1)\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    return save_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d3cee2",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd91a3f5",
   "metadata": {},
   "source": [
    "We will apply data augmentation to increase the dataset. We will use data augmentation per epoch in order to generate unique images in order to increase the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa260c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T18:54:48.774030Z",
     "start_time": "2021-09-28T18:54:48.759279Z"
    }
   },
   "outputs": [],
   "source": [
    "resnet50 = keras.applications.resnet50\n",
    "\n",
    "imgdatagen = ImageDataGenerator(\n",
    "        preprocessing_function = resnet50.preprocess_input,\n",
    "        horizontal_flip = True,\n",
    "        vertical_flip = True,\n",
    "        width_shift_range = 10,\n",
    "        height_shift_range = 10,\n",
    "        rotation_range = 5,\n",
    "        brightness_range = [0.8,1.3],\n",
    "        fill_mode = \"nearest\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e6754",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362d8a8f",
   "metadata": {},
   "source": [
    "The model will be based on ResNet50 Architecture. The classifier block will be changed to suit to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb57ebbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T18:54:48.789938Z",
     "start_time": "2021-09-28T18:54:48.777813Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    conv_model = resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "    for layer in conv_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = keras.layers.GlobalAveragePooling2D()(conv_model.output)\n",
    "    x = keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    predictions = keras.layers.Dense(3, activation='softmax')(x)\n",
    "\n",
    "    full_model = keras.models.Model(inputs=conv_model.input, outputs=predictions)\n",
    "\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a7a6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T18:54:50.784010Z",
     "start_time": "2021-09-28T18:54:48.790888Z"
    }
   },
   "outputs": [],
   "source": [
    "full_model = create_model()\n",
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0da3af",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855eebf2",
   "metadata": {},
   "source": [
    "We will check the combination of hyperparameters that yielded the highest validation accuracy and lowest validation loss. A set of values on chosen hyperparameters is given. The best combination will be used to train the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4879c732",
   "metadata": {},
   "source": [
    "### Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6cbc49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T18:54:50.800784Z",
     "start_time": "2021-09-28T18:54:50.786539Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size_arr = [8]                                #batch size\n",
    "num_epochs_arr = [110]                              #number of epochs\n",
    "lr_arr = [0.0001]                                   #learning rate Adamax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a689a9eb",
   "metadata": {},
   "source": [
    "### Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb7e62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T18:54:50.815849Z",
     "start_time": "2021-09-28T18:54:50.801853Z"
    }
   },
   "outputs": [],
   "source": [
    "VALIDATION_ACCURACY = []                          #holds all validation_accuracy in current combination\n",
    "VALIDATION_LOSS = []                              #holds all validation_loss in current combination\n",
    "\n",
    "TRAIN_ACCURACY = []                               #holds all train_accuracy in current combination\n",
    "TRAIN_LOSS = []                                   #holds all train_loss in current combination\n",
    "\n",
    "TEST_ACCURACY = []                                #holds all test_accuracy in current combination\n",
    "TEST_LOSS = []                                    #holds all test_loss in current combination\n",
    "\n",
    "param_arr = []                                    #holds the hyperparameter values per combination\n",
    "\n",
    "val_acc_arr = []                                  #holds the set of validation accuracy per combination\n",
    "val_loss_arr = []                                 #holds the set of validation loss per combination\n",
    "val_acc_means = []                                #holds the means of all set of validation accuracy per combination\n",
    "val_loss_means = []                               #holds the means of all set of validation loss per combination\n",
    "\n",
    "train_acc_arr = []                                #holds the set of train accuracy per combination\n",
    "train_loss_arr = []                               #holds the set of train loss per combination\n",
    "train_acc_means = []                              #holds the means of all set of train accuracy per combination\n",
    "train_loss_means = []                             #holds the means of all set of train loss per combination\n",
    "\n",
    "test_acc_arr = []                                 #holds the set of test accuracy per combination\n",
    "test_loss_arr = []                                #holds the set of test loss per combination\n",
    "test_acc_means = []                               #holds the means of all set of test accuracy per combination\n",
    "test_loss_means = []                              #holds the means of all set of test loss per combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af299f30",
   "metadata": {},
   "source": [
    "### Train, Validation, Test data settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339830ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T18:54:51.065668Z",
     "start_time": "2021-09-28T18:54:50.816813Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dir = directory + \"/ToBeUsed/train_set\"     #where the train set images are stored\n",
    "\n",
    "#strat kfold parameters\n",
    "Y = df2[['label']]\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = 69, shuffle = True)\n",
    "\n",
    "#to get the test set images convert the image to numpy array and store to testX\n",
    "testX = []\n",
    "\n",
    "for img_path in X_test['filename']:\n",
    "    img = im.load_img(img_path, target_size=(224,224))\n",
    "    x = im.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    x = x.reshape(224,224,3)\n",
    "    testX.append(x)\n",
    "\n",
    "testX = np.array(testX)\n",
    "\n",
    "yt = pd.to_numeric(y_test['label'])                #make the labels to integer to get the model metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d18005",
   "metadata": {},
   "source": [
    "Train the model per combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97303874",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T02:44:21.153943Z",
     "start_time": "2021-09-28T18:54:51.066606Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in range(len(batch_size_arr)):\n",
    "    \n",
    "    #for batch size values\n",
    "    if len(param_arr) == 0:\n",
    "        param_arr.append([])\n",
    "    \n",
    "    for y in range(len(num_epochs_arr)):\n",
    "\n",
    "        #for epoch values\n",
    "        if len(param_arr) == 1:\n",
    "            param_arr.append([])\n",
    "        \n",
    "        for z in range(len(lr_arr)):\n",
    "            \n",
    "            #for learning rate values\n",
    "            if len(param_arr) == 2:\n",
    "                param_arr.append([])\n",
    "               \n",
    "            print('')\n",
    "            print(\"batch size: \", batch_size_arr[x])\n",
    "            print(\"epochs: \", num_epochs_arr[y])\n",
    "            print(\"learning rate: \", lr_arr[z])\n",
    "            print('')\n",
    "            \n",
    "            #append the values per combo\n",
    "            param_arr[0].append(batch_size_arr[x])\n",
    "            param_arr[1].append(num_epochs_arr[y])\n",
    "            param_arr[2].append(lr_arr[z])\n",
    "            \n",
    "            VALIDATION_ACCURACY = []\n",
    "            VALIDATION_LOSS = []\n",
    "            \n",
    "            TRAIN_ACCURACY = []\n",
    "            TRAIN_LOSS = []\n",
    "            \n",
    "            TEST_ACCURACY = []\n",
    "            TEST_LOSS = []\n",
    "            \n",
    "            #change save_dir value per combination\n",
    "            save_dir = savemodel()\n",
    "            fold_var = 1\n",
    "            \n",
    "            for train_index, val_index in skf.split(np.zeros(len(df2)),Y):\n",
    "                \n",
    "                #continuously split the dataset to train set and validation set in n folds\n",
    "                training_data = df2.iloc[train_index]\n",
    "                validation_data = df2.iloc[val_index]\n",
    "                \n",
    "                print('')\n",
    "                print(\"Fold number \" + str(fold_var))\n",
    "                \n",
    "                #apply data augmentation to both train set and validation set\n",
    "                train_data_generator = imgdatagen.flow_from_dataframe(training_data, directory = train_dir,\n",
    "                                       x_col = \"filename\", y_col = \"label\",\n",
    "                                       class_mode = \"categorical\", target_size = (224,224), batch_size = batch_size_arr[x],\n",
    "                                            shuffle = True)\n",
    "                valid_data_generator  = imgdatagen.flow_from_dataframe(validation_data, directory = train_dir,\n",
    "                                        x_col = \"filename\", y_col = \"label\",\n",
    "                                        class_mode = \"categorical\", target_size = (224,224), batch_size = batch_size_arr[x],\n",
    "                                            shuffle = True)\n",
    "                \n",
    "                #use the created model\n",
    "                full_model = create_model()\n",
    "                \n",
    "                #compile the model\n",
    "                full_model.compile(loss='categorical_crossentropy',\n",
    "                              optimizer=keras.optimizers.Adamax(learning_rate=lr_arr[z]),\n",
    "                              metrics=['accuracy']) #categorical_crossentropy cause it is more than 2 classes\n",
    "\n",
    "                #create callbacks\n",
    "                checkpoint = tf.keras.callbacks.ModelCheckpoint((save_dir+\"/\"+ \"model_\" + str(fold_var) + \".h5\"), \n",
    "                                        monitor='val_accuracy', verbose=1, \n",
    "                                        save_best_only=True, mode='max')\n",
    "                callbacks_list = [checkpoint]\n",
    "                #this saves the best model\n",
    "\n",
    "\n",
    "                #fit the model\n",
    "                history = full_model.fit(train_data_generator,\n",
    "                                         epochs=num_epochs_arr[y],\n",
    "                                         callbacks=callbacks_list,\n",
    "                                         validation_data=valid_data_generator,\n",
    "                                         verbose=1)\n",
    "\n",
    "                #plot the history\n",
    "                plot_history(history, yrange=(0.2,1))\n",
    "\n",
    "                #load the best model instance to evaluate the performance of the model\n",
    "                full_model.load_weights(save_dir+\"/model_\"+str(fold_var)+\".h5\")\n",
    "\n",
    "                results = full_model.evaluate(valid_data_generator)\n",
    "                results = dict(zip(full_model.metrics_names,results))\n",
    "                \n",
    "                #store Validation accuracy/loss\n",
    "                VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "                VALIDATION_LOSS.append(results['loss'])\n",
    "                \n",
    "                #store Train accuracy/loss\n",
    "                TRAIN_ACCURACY.append(np.mean(history.history['accuracy']))\n",
    "                TRAIN_LOSS.append(np.mean(history.history['loss']))\n",
    "                \n",
    "                #predict test set to get the test accuracy/loss\n",
    "                yhat_probs = full_model.predict(testX, verbose=0) \n",
    "                yhat_classes = yhat_probs.argmax(axis=-1)\n",
    "                \n",
    "                #test accuracy and loss score\n",
    "                accuracy = accuracy_score(yt, yhat_classes)\n",
    "                loss = log_loss(yt, yhat_probs)\n",
    "                \n",
    "                #store Test accuracy and loss\n",
    "                TEST_ACCURACY.append(accuracy)\n",
    "                TEST_LOSS.append(loss)\n",
    "                \n",
    "                tf.keras.backend.clear_session()\n",
    "\n",
    "                fold_var += 1\n",
    "            \n",
    "            #store the all accuracy and loss in all folds per combination\n",
    "            val_acc_arr.append(VALIDATION_ACCURACY)\n",
    "            val_loss_arr.append(VALIDATION_LOSS)\n",
    "            \n",
    "            train_acc_arr.append(TRAIN_ACCURACY)                            \n",
    "            train_loss_arr.append(TRAIN_LOSS)\n",
    "            \n",
    "            test_acc_arr.append(TEST_ACCURACY)\n",
    "            test_loss_arr.append(TEST_LOSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b537b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b8157",
   "metadata": {},
   "source": [
    "### Display Values Per Combination of Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbfe699",
   "metadata": {},
   "source": [
    "We will put the combination of hyperparameters and the resulting means of accuracy and loss together in a dataframe in order to display it nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa720823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T14:31:32.077186Z",
     "start_time": "2021-09-29T14:31:32.017869Z"
    }
   },
   "outputs": [],
   "source": [
    "#append means of accuracy and loss per combo\n",
    "for x in range(len(val_acc_arr)):\n",
    "    val_acc_means.append(np.mean(val_acc_arr[x]))                                \n",
    "    val_loss_means.append(np.mean(val_loss_arr[x]))\n",
    "    \n",
    "    train_acc_means.append(np.mean(train_acc_arr[x]))                              \n",
    "    train_loss_means.append(np.mean(train_loss_arr[x]))                             \n",
    "\n",
    "    test_acc_means.append(np.mean(test_acc_arr[x]))                               \n",
    "    test_loss_means.append(np.mean(test_loss_arr[x])) \n",
    "    \n",
    "df_hyperparam = pd.DataFrame(list(zip(param_arr[0],param_arr[1],param_arr[2],\n",
    "                                      train_acc_means, train_loss_means,\n",
    "                                      val_acc_means,val_loss_means,\n",
    "                                      test_acc_means, test_loss_means))\n",
    "                             ,columns=['batch size','number of epoch', 'learning rate', \n",
    "                                       'train accuracy', 'train loss',\n",
    "                                       'val accuracy', 'val loss',\n",
    "                                       'test accuracy', 'test loss'])\n",
    "\n",
    "df_hyperparam = df_hyperparam.sort_values(ascending=False,by = 'val accuracy')\n",
    "\n",
    "#df_hyperparam.to_excel(\"hyperparam_result.xlsx\")\n",
    "\n",
    "df_hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be7afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44439d90",
   "metadata": {},
   "source": [
    "### Each fold results of best combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67806e39",
   "metadata": {},
   "source": [
    "Below are the validation accuracy and validation loss per fold and their average of the best combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2b50e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T16:41:32.356102Z",
     "start_time": "2021-09-29T16:41:32.320718Z"
    }
   },
   "outputs": [],
   "source": [
    "#get the index of the best combination\n",
    "comb_no = df_hyperparam.loc[[df_hyperparam['val accuracy'].idxmax()]].index.tolist()[0]\n",
    "fold_num = [*range(1,6)]\n",
    "\n",
    "fold_eval = pd.DataFrame(list(zip(fold_num,train_acc_arr[comb_no],train_loss_arr[comb_no],\n",
    "                                  val_acc_arr[comb_no],val_loss_arr[comb_no],\n",
    "                                  test_acc_arr[comb_no],test_loss_arr[comb_no]))\n",
    "                         ,columns=['fold','Training Accuracy','Training Loss',\n",
    "                                   'Validation Accuracy','Validation Loss',\n",
    "                                   'Test Accuracy','Test Loss'])\n",
    "\n",
    "fold_eval.set_index('fold', inplace=True)\n",
    "fold_eval.loc['mean'] = fold_eval.mean()\n",
    "\n",
    "#fold_eval.to_excel('fold_eval.xlsx')\n",
    "fold_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b2913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453efa14",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c699e156",
   "metadata": {},
   "source": [
    "We will use the best combination of hypaparameters to train our final model. We will evaluate the model using the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d833784",
   "metadata": {},
   "source": [
    "### Get best combination of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa978783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bestohypaparamu = df_hyperparam.loc[[df_hyperparam['val accuracy'].idxmax()]]\n",
    "\n",
    "bestohypaparamu = [df_bestohypaparamu['batch size'].values[0],\n",
    "                   df_bestohypaparamu['number of epoch'].values[0],\n",
    "                   df_bestohypaparamu['learning rate'].values[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a75fd75",
   "metadata": {},
   "source": [
    "### Train and Predict the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfd2613",
   "metadata": {},
   "source": [
    "The same as to what was done in the hyperparameter tuning, we will split the train set to train and validation set with the same random state of 69 in order to have a similar resulting accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2535ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train set to train set and valid set\n",
    "X = df2[['filename']]\n",
    "y = df2[['label']]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=69)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a988202a",
   "metadata": {},
   "source": [
    "We will save the images from the new train set and validation set to separate folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad93bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder to store the train and test set folders\n",
    "fortraintest2 = directory + \"/ToBeUsedFinalModel\"\n",
    "if not os.path.exists(fortraintest2):\n",
    "    os.makedirs(fortraintest2)\n",
    "\n",
    "#train set folder\n",
    "tr2 = fortraintest2 + \"/train_set\"\n",
    "if not os.path.exists(tr2):\n",
    "    os.makedirs(tr2)\n",
    "else:\n",
    "    #remove previous folder and create new one\n",
    "    shutil.rmtree(tr2)\n",
    "    os.makedirs(tr2)\n",
    "    \n",
    "#test set folder\n",
    "va = fortraintest2 + \"/val_set\"\n",
    "if not os.path.exists(va):\n",
    "    os.makedirs(va)\n",
    "else:\n",
    "    #remove previous folder and create new one\n",
    "    shutil.rmtree(va)\n",
    "    os.makedirs(va)\n",
    "\n",
    "#copying the images to the train set folder\n",
    "for index, row in X_train.iterrows():\n",
    "    trcpy = shutil.copy2(row['filename'], tr2)\n",
    "    X_train.loc[X_train['filename']==row['filename'],'filename']=trcpy #change to directory where image is copied\n",
    "    \n",
    "#copying the images to the test set folder\n",
    "for index, row in X_val.iterrows():\n",
    "    vacpy = shutil.copy2(row['filename'], va)\n",
    "    X_val.loc[X_val['filename']==row['filename'],'filename']=vacpy #change to directory where image is copied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ebfd4a",
   "metadata": {},
   "source": [
    "### Train Set Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f5cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.concat([X_train,y_train],axis=1)\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77530fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "counto = [0,0,0]\n",
    "\n",
    "counto_count = 0\n",
    "\n",
    "for x in df4['label'].unique():\n",
    "    for y in df4['label']:\n",
    "        if x == y:\n",
    "            if counto_count == 0:\n",
    "                counto[0] += 1\n",
    "            elif counto_count == 1:\n",
    "                counto[1] += 1\n",
    "            else:\n",
    "                counto[2] += 1\n",
    "                \n",
    "    counto_count += 1\n",
    "    \n",
    "print(counto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e782e4c7",
   "metadata": {},
   "source": [
    "### Validation Set Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba63df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.concat([X_val,y_val],axis=1)\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1512a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "counto = [0,0,0]\n",
    "\n",
    "counto_count = 0\n",
    "\n",
    "for x in df5['label'].unique():\n",
    "    for y in df5['label']:\n",
    "        if x == y:\n",
    "            if counto_count == 0:\n",
    "                counto[0] += 1\n",
    "            elif counto_count == 1:\n",
    "                counto[1] += 1\n",
    "            else:\n",
    "                counto[2] += 1\n",
    "                \n",
    "    counto_count += 1\n",
    "    \n",
    "print(counto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6025ec5",
   "metadata": {},
   "source": [
    "Finally we train the model. Prediction is also done using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c634d83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VAL_ACCURACY = None\n",
    "VAL_LOSS = None\n",
    "\n",
    "train_dir = tr2\n",
    "valid_dir = va\n",
    "save_dir = gen_dir + \"/saved_finalmodel_1\"\n",
    "\n",
    "if os.path.exists(save_dir):\n",
    "    shutil.rmtree(save_dir)\n",
    "    \n",
    "os.makedirs(save_dir)\n",
    "\n",
    "trainu = df4\n",
    "validu = df5\n",
    "\n",
    "train_data_generator = imgdatagen.flow_from_dataframe(trainu, directory = train_dir,\n",
    "                                       x_col = \"filename\", y_col = \"label\",\n",
    "                                       class_mode = \"categorical\", target_size = (224,224), batch_size = bestohypaparamu[0],\n",
    "                                            shuffle = True)\n",
    "valid_data_generator  = imgdatagen.flow_from_dataframe(validu, directory = valid_dir,\n",
    "                        x_col = \"filename\", y_col = \"label\",\n",
    "                        class_mode = \"categorical\", target_size = (224,224), batch_size = bestohypaparamu[0],\n",
    "                            shuffle = True)\n",
    "\n",
    "#print(len(train_data_generator))\n",
    "\n",
    "full_model = create_model()\n",
    "# Compile the model\n",
    "full_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adamax(learning_rate=bestohypaparamu[2]),\n",
    "              metrics=['accuracy']) #categorical_crossentropy cause it is more than 2 classes\n",
    "\n",
    "# Create callbacks\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint((save_dir+\"/\"+ \"finalmodel_1.h5\"), \n",
    "                        monitor='val_accuracy', verbose=1, \n",
    "                        save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "# This saves the best model\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "history = full_model.fit(train_data_generator,\n",
    "                         epochs=bestohypaparamu[1],\n",
    "                         callbacks=callbacks_list,\n",
    "                         validation_data=valid_data_generator,\n",
    "                         verbose=1)\n",
    "\n",
    "# Plot the history\n",
    "plot_history(history, yrange=(0.2,1))\n",
    "\n",
    "# Load the best model instance to evaluate the performance of the model\n",
    "full_model.load_weights(save_dir+\"/\"+ \"finalmodel_1.h5\")\n",
    "\n",
    "results = full_model.evaluate(valid_data_generator)\n",
    "\n",
    "#print(results)\n",
    "\n",
    "results = dict(zip(full_model.metrics_names,results))\n",
    "\n",
    "# Store Validation accuracy/loss\n",
    "VAL_ACCURACY = results['accuracy']\n",
    "VAL_LOSS = results['loss']\n",
    "\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c39718",
   "metadata": {},
   "source": [
    "Display the validation accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679460c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Val acc:  \" + str(VAL_ACCURACY))\n",
    "print(\"Val loss: \" + str(VAL_LOSS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f206122",
   "metadata": {},
   "source": [
    "Predict using test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ffa08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = gen_dir + \"/saved_finalmodel_1\"\n",
    "full_model = create_model()\n",
    "full_model.load_weights(save_dir+\"/\"+ \"finalmodel_1.h5\")\n",
    "\n",
    "# Needed later for measuring performance metrics\n",
    "# Predict test set to get the test accuracy/loss\n",
    "yhat_probs2 = full_model.predict(testX, verbose=0) \n",
    "yhat_classes2 = yhat_probs2.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07cc304",
   "metadata": {},
   "source": [
    "## Measure Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc454dd1",
   "metadata": {},
   "source": [
    "After we have trained the final model with the entire train set and done prediction using the test set, we can now measure the performance metrics for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metrics\n",
    "def evaluato(yhat_probs2, \n",
    "             yhat_classes2 \n",
    "            ):\n",
    "\n",
    "    # accuracy: (tp + tn) / (p + n)\n",
    "    matrix = confusion_matrix(yt, yhat_classes2)\n",
    "    accuracy2 = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    print('Accuracy per class: ' + str(accuracy2))\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(yt, yhat_classes2, average=\"weighted\")\n",
    "    print('Precision: %f' % precision)\n",
    "    precision2 = precision_score(yt, yhat_classes2, average=None)\n",
    "    print('Precision per class: ' + str(precision2))\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(yt, yhat_classes2, average=\"weighted\")\n",
    "    print('Recall: %f' % recall)\n",
    "    recall2 = recall_score(yt, yhat_classes2, average=None)\n",
    "    print('Recall per class: ' + str(recall2))\n",
    "    \n",
    "    \n",
    "    #ROC AUC\n",
    "    #-------------------------------------------------------------------------------\n",
    "    # Binarize the true label\n",
    "    ytbin = label_binarize(yt, classes=[0, 1, 2])\n",
    "    n_classes = ytbin.shape[1]\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "      \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(ytbin[:, i], yhat_probs2[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i]) \n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"macro\"], tpr[\"macro\"], _ = roc_curve(ytbin.ravel(), yhat_probs2.ravel())\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    plt.rcParams['axes.facecolor'] = 'white'\n",
    "    plt.rcParams[\"axes.edgecolor\"] = 'black'\n",
    "    plt.rcParams[\"axes.linewidth\"]  = 1.25\n",
    "    \n",
    "    #balut-penoy\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr[0], tpr[0], color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.4f)' % roc_auc[0])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Balut-Penoy')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    #salted egg\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr[1], tpr[1], color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.4f)' % roc_auc[1])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Salted Egg')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    #table egg\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.4f)' % roc_auc[2])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Table Egg')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    #-------------------------------------------------------------------------------\n",
    "    \n",
    "    #confusion matrix\n",
    "    matrix = confusion_matrix(yt, yhat_classes2)\n",
    "    \n",
    "    df_cm = pd.DataFrame(matrix, range(matrix.shape[0]), range(matrix.shape[1]))\n",
    "    df_cm = df_cm.rename(columns={0:'Balut/Penoy', 1:'Salted Egg', 2:'Table Egg'}, \n",
    "                         index={0:'Balut/Penoy', 1:'Salted Egg', 2:'Table Egg'})\n",
    "    \n",
    "    sn.set(font_scale=1.4) # for label size\n",
    "    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title('Confusion Matrix of Final Model Evaluation')\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy2, precision2, recall2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c4358d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy2, precision2, recall2 = evaluato(yhat_probs2, yhat_classes2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2237ea17",
   "metadata": {},
   "source": [
    "### Metrics per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39951e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy2 = list(accuracy2)\n",
    "precision2 = list(precision2)\n",
    "recall2 = list(recall2)\n",
    "\n",
    "accuracy2.append(0)\n",
    "precision2.append(0)\n",
    "recall2.append(0)\n",
    "\n",
    "classes = 3\n",
    "\n",
    "for x in range(classes):\n",
    "    accuracy2[-1] += accuracy2[x]\n",
    "    precision2[-1] += precision2[x]\n",
    "    recall2[-1] += recall2[x]\n",
    "\n",
    "accuracy2[-1] = accuracy2[-1]/classes\n",
    "precision2[-1] = precision2[-1]/classes\n",
    "recall2[-1] = recall2[-1]/classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11abfab7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label = ['Balut/Penoy','Salted','Table','Ave']\n",
    "\n",
    "df_eval_per_class = pd.DataFrame(data = list(zip(label,accuracy2,precision2,recall2)), \n",
    "                       columns=[\"Class\",\"Accuracy\",\"Precision\",\"Recall\"])\n",
    "\n",
    "#set fold_ave as the index\n",
    "df_eval_per_class.set_index(\"Class\",inplace=True)\n",
    "\n",
    "df_eval_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b7b9bf",
   "metadata": {},
   "source": [
    "## Validating Misclassified Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cca65e",
   "metadata": {},
   "source": [
    "We will display the correct and incorrect predictions. We will first get the images from the directories stored under the filename column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f4b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testX2 = []\n",
    "\n",
    "for img_path in X_test['filename']:\n",
    "    img = cv2.imread(img_path)\n",
    "    testX2.append(img)\n",
    "\n",
    "testX2 = np.array(testX2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f2738",
   "metadata": {},
   "source": [
    "We will be applying Grad-CAM in order to check where is the focus of the model. It presents a heatmap. The hotter the color on a region is, the more it is focused on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42b131",
   "metadata": {},
   "source": [
    "### Grad-CAM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c57ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_layer_name = \"conv5_block3_add\"\n",
    "\n",
    "preprocess_input = keras.applications.resnet50.preprocess_input\n",
    "\n",
    "def get_img_array(img_path, size):\n",
    "    # `img` is a PIL image of size 224x224\n",
    "    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n",
    "    # `array` is a float32 Numpy array of shape (224, 224, 3)\n",
    "    array = keras.preprocessing.image.img_to_array(img)\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    # of size (1, 224, 224, 3)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c34e2d",
   "metadata": {},
   "source": [
    "### superimposed visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd5d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.5):\n",
    "    # Load the original image\n",
    "    img = keras.preprocessing.image.load_img(img_path)\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "    \n",
    "    # Save the superimposed image\n",
    "    superimposed_img.save(cam_path)\n",
    "\n",
    "    # Display Grad CAM\n",
    "    display(Image(cam_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c49f96",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c2dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guraducamu(img_dir):\n",
    "    img_size = (224,224)\n",
    "\n",
    "    # Prepare image\n",
    "    img_array = get_img_array(img_dir, img_size)\n",
    "\n",
    "    conv_model = keras.applications.resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "\n",
    "    for layer in conv_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = keras.layers.GlobalAveragePooling2D()(conv_model.output)\n",
    "    x = keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    predictions = keras.layers.Dense(3, activation='softmax',name = \"prediction\")(x)\n",
    "\n",
    "    model = keras.models.Model(inputs=conv_model.input, outputs=predictions)\n",
    "\n",
    "    model.load_weights(save_dir+\"/\"+ \"finalmodel_1.h5\")\n",
    "\n",
    "    # Remove last layer's softmax\n",
    "    model.layers[-1].activation = None\n",
    "\n",
    "    # Generate class activation heatmap\n",
    "    heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
    " \n",
    "    print(\"Original Image\")\n",
    "    display(Image(img_dir))\n",
    "\n",
    "    print(\"Heatmap Prediction\")\n",
    "    plt.matshow(heatmap)\n",
    "    plt.grid(False)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Grad-CAM\")\n",
    "    save_and_display_gradcam(img_dir, heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56647ac6",
   "metadata": {},
   "source": [
    "### Correct predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bcf3c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T16:44:35.361689Z",
     "start_time": "2021-09-29T16:44:32.438363Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct = np.where(yhat_classes2==yt)[0]\n",
    "\n",
    "print(\"Found %d correct labels\" % len(correct))\n",
    "\n",
    "for x in correct:\n",
    "    print(\"Predict {0}, Class {1}\".format(str(yhat_classes2[x]),str(yt.values[x])))\n",
    "    img_dir = X_test.iloc[x]['filename']\n",
    "    print(img_dir)\n",
    "    guraducamu(img_dir)\n",
    "    \n",
    "plt.figure(figsize=(20,60))\n",
    "for i, correct in enumerate(correct[:6]):\n",
    "    plt.subplot(3,2,i+1)\n",
    "    RGB_img = cv2.cvtColor(testX2[correct], cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(RGB_img) #, interpolation='none') #, cmap='gray')\n",
    "    plt.title(\"Predict {0}, Class {1}\".format(str(yhat_classes2[correct]),str(yt.values[correct])))\n",
    "    plt.grid(False)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d635019d",
   "metadata": {},
   "source": [
    "### Incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce587acd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-29T16:44:38.293051Z",
     "start_time": "2021-09-29T16:44:35.810381Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "incorrect = np.where(yhat_classes2!=yt)[0]\n",
    "\n",
    "print (\"Found %d incorrect labels\" % len(incorrect))\n",
    "\n",
    "for x in incorrect:\n",
    "    print(\"Predict {0}, Class {1}\".format(str(yhat_classes2[x]),str(yt.values[x])))\n",
    "    img_dir = X_test.iloc[x]['filename']\n",
    "    print(img_dir)\n",
    "    guraducamu(img_dir)\n",
    "\n",
    "plt.figure(figsize=(20,60))\n",
    "for i, incorrect in enumerate(incorrect[:6]):\n",
    "    plt.subplot(3,2,i+1)\n",
    "    RGB_img = cv2.cvtColor(testX2[incorrect], cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(RGB_img) #, interpolation='none') #, cmap='gray')\n",
    "    plt.title(\"Predicted {0}, Class {1}\".format(str(yhat_classes2[incorrect]), str(yt.values[incorrect])))\n",
    "    plt.grid(False)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0160b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
